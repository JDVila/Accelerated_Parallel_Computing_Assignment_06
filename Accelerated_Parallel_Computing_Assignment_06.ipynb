{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOdU9iqWwabRqCh5VYsAvej",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JDVila/Accelerated_Parallel_Computing_Assignment_06/blob/main/Accelerated_Parallel_Computing_Assignment_06.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvJ8T3Zohu1d",
        "outputId": "7d5083f5-f0a3-4c8e-95fb-ee2cb86cfebb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f94PpaOKjSul",
        "outputId": "67fe3ad7-3518-47ec-9f1a-94daf9f41ef9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-4lno37vh\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-4lno37vh\n",
            "  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit 0a71d56e5dce3ff1f0dd2c47c29367629262f527\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-py3-none-any.whl size=4295 sha256=e1e9e3d545bc30cb94713d91ebcc974ee2107a76499e183271f8bbc855709f86\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-k58qh6_r/wheels/a8/b9/18/23f8ef71ceb0f63297dd1903aedd067e6243a68ea756d6feea\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext nvcc_plugin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tiqhoF0ejTsN",
        "outputId": "1a21ee5f-b8ff-41f7-dbad-1177e1e75b4f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "#include <stdio.h>\n",
        "\n",
        "void helloCPU()\n",
        "{\n",
        "  printf(\"Hello from the CPU.\\n\");\n",
        "}\n",
        "\n",
        "/*\n",
        " * The addition of `__global__` signifies that this function\n",
        " * should be launced on the GPU.\n",
        " */\n",
        "\n",
        "__global__ void helloGPU()\n",
        "{\n",
        "  printf(\"Hello from the GPU.\\n\");\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "  int x= 3;\n",
        "  helloCPU();\n",
        "\n",
        "\n",
        "  /*\n",
        "   * Add an execution configuration with the <<<...>>> syntax\n",
        "   * will launch this function as a kernel on the GPU.\n",
        "   */\n",
        "\n",
        "  helloGPU<<<1, 1>>>();\n",
        "\n",
        "  /*\n",
        "   * `cudaDeviceSynchronize` will block the CPU stream until\n",
        "   * all GPU kernels have completed.\n",
        "   */\n",
        "\n",
        "\n",
        "  cudaDeviceSynchronize();\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGf0PtN3greI",
        "outputId": "bc6947b6-367d-4d17-93ac-4c0b870c55dd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello from the CPU.\n",
            "Hello from the GPU.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "#include <stdio.h>\n",
        "\n",
        "__global__\n",
        "void saxpy(int n, float a, float *x, float *y)\n",
        "{\n",
        "  int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  printf(\"GPU active.....Thread %i\\n\", i);\n",
        "  if (i < n) y[i] = a * x[i] + y[i];\n",
        "}\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "  int N = 100;\n",
        "  float *x, *y, *d_x, *d_y;\n",
        "  x = (float*)malloc(N*sizeof(float));\n",
        "  y = (float*)malloc(N*sizeof(float));\n",
        "\n",
        "  cudaMalloc(&d_x, N*sizeof(float));\n",
        "  cudaMalloc(&d_y, N*sizeof(float));\n",
        "\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    x[i] = i + 0.0f;\n",
        "    y[i] = i * 2.0f;\n",
        "  }\n",
        "\n",
        "  cudaMemcpy(d_x, x, N*sizeof(float), cudaMemcpyHostToDevice);\n",
        "  cudaMemcpy(d_y, y, N*sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "  // Perform SAXPY on 100 elements\n",
        "  saxpy<<<(N+255)/256, 256>>>(N, 2.0f, d_x, d_y);\n",
        "\n",
        "  cudaMemcpy(y, d_y, N*sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "  float maxError = 0.0f;\n",
        "  for (int i = 0; i < N; i++)\n",
        "    maxError = max(maxError, abs(y[i]-4.0f));\n",
        "  printf(\"Max error: %f\\n\", maxError);\n",
        "\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    printf(\"%.2f\\n\", y[i]);\n",
        "  }\n",
        "\n",
        "  cudaFree(d_x);\n",
        "  cudaFree(d_y);\n",
        "  free(x);\n",
        "  free(y);\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfdLt255kpaT",
        "outputId": "0e6ce75e-cca2-4014-fd32-8e223d40a12d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU active.....Thread 64\n",
            "GPU active.....Thread 65\n",
            "GPU active.....Thread 66\n",
            "GPU active.....Thread 67\n",
            "GPU active.....Thread 68\n",
            "GPU active.....Thread 69\n",
            "GPU active.....Thread 70\n",
            "GPU active.....Thread 71\n",
            "GPU active.....Thread 72\n",
            "GPU active.....Thread 73\n",
            "GPU active.....Thread 74\n",
            "GPU active.....Thread 75\n",
            "GPU active.....Thread 76\n",
            "GPU active.....Thread 77\n",
            "GPU active.....Thread 78\n",
            "GPU active.....Thread 79\n",
            "GPU active.....Thread 80\n",
            "GPU active.....Thread 81\n",
            "GPU active.....Thread 82\n",
            "GPU active.....Thread 83\n",
            "GPU active.....Thread 84\n",
            "GPU active.....Thread 85\n",
            "GPU active.....Thread 86\n",
            "GPU active.....Thread 87\n",
            "GPU active.....Thread 88\n",
            "GPU active.....Thread 89\n",
            "GPU active.....Thread 90\n",
            "GPU active.....Thread 91\n",
            "GPU active.....Thread 92\n",
            "GPU active.....Thread 93\n",
            "GPU active.....Thread 94\n",
            "GPU active.....Thread 95\n",
            "GPU active.....Thread 96\n",
            "GPU active.....Thread 97\n",
            "GPU active.....Thread 98\n",
            "GPU active.....Thread 99\n",
            "GPU active.....Thread 100\n",
            "GPU active.....Thread 101\n",
            "GPU active.....Thread 102\n",
            "GPU active.....Thread 103\n",
            "GPU active.....Thread 104\n",
            "GPU active.....Thread 105\n",
            "GPU active.....Thread 106\n",
            "GPU active.....Thread 107\n",
            "GPU active.....Thread 108\n",
            "GPU active.....Thread 109\n",
            "GPU active.....Thread 110\n",
            "GPU active.....Thread 111\n",
            "GPU active.....Thread 112\n",
            "GPU active.....Thread 113\n",
            "GPU active.....Thread 114\n",
            "GPU active.....Thread 115\n",
            "GPU active.....Thread 116\n",
            "GPU active.....Thread 117\n",
            "GPU active.....Thread 118\n",
            "GPU active.....Thread 119\n",
            "GPU active.....Thread 120\n",
            "GPU active.....Thread 121\n",
            "GPU active.....Thread 122\n",
            "GPU active.....Thread 123\n",
            "GPU active.....Thread 124\n",
            "GPU active.....Thread 125\n",
            "GPU active.....Thread 126\n",
            "GPU active.....Thread 127\n",
            "GPU active.....Thread 128\n",
            "GPU active.....Thread 129\n",
            "GPU active.....Thread 130\n",
            "GPU active.....Thread 131\n",
            "GPU active.....Thread 132\n",
            "GPU active.....Thread 133\n",
            "GPU active.....Thread 134\n",
            "GPU active.....Thread 135\n",
            "GPU active.....Thread 136\n",
            "GPU active.....Thread 137\n",
            "GPU active.....Thread 138\n",
            "GPU active.....Thread 139\n",
            "GPU active.....Thread 140\n",
            "GPU active.....Thread 141\n",
            "GPU active.....Thread 142\n",
            "GPU active.....Thread 143\n",
            "GPU active.....Thread 144\n",
            "GPU active.....Thread 145\n",
            "GPU active.....Thread 146\n",
            "GPU active.....Thread 147\n",
            "GPU active.....Thread 148\n",
            "GPU active.....Thread 149\n",
            "GPU active.....Thread 150\n",
            "GPU active.....Thread 151\n",
            "GPU active.....Thread 152\n",
            "GPU active.....Thread 153\n",
            "GPU active.....Thread 154\n",
            "GPU active.....Thread 155\n",
            "GPU active.....Thread 156\n",
            "GPU active.....Thread 157\n",
            "GPU active.....Thread 158\n",
            "GPU active.....Thread 159\n",
            "GPU active.....Thread 160\n",
            "GPU active.....Thread 161\n",
            "GPU active.....Thread 162\n",
            "GPU active.....Thread 163\n",
            "GPU active.....Thread 164\n",
            "GPU active.....Thread 165\n",
            "GPU active.....Thread 166\n",
            "GPU active.....Thread 167\n",
            "GPU active.....Thread 168\n",
            "GPU active.....Thread 169\n",
            "GPU active.....Thread 170\n",
            "GPU active.....Thread 171\n",
            "GPU active.....Thread 172\n",
            "GPU active.....Thread 173\n",
            "GPU active.....Thread 174\n",
            "GPU active.....Thread 175\n",
            "GPU active.....Thread 176\n",
            "GPU active.....Thread 177\n",
            "GPU active.....Thread 178\n",
            "GPU active.....Thread 179\n",
            "GPU active.....Thread 180\n",
            "GPU active.....Thread 181\n",
            "GPU active.....Thread 182\n",
            "GPU active.....Thread 183\n",
            "GPU active.....Thread 184\n",
            "GPU active.....Thread 185\n",
            "GPU active.....Thread 186\n",
            "GPU active.....Thread 187\n",
            "GPU active.....Thread 188\n",
            "GPU active.....Thread 189\n",
            "GPU active.....Thread 190\n",
            "GPU active.....Thread 191\n",
            "GPU active.....Thread 192\n",
            "GPU active.....Thread 193\n",
            "GPU active.....Thread 194\n",
            "GPU active.....Thread 195\n",
            "GPU active.....Thread 196\n",
            "GPU active.....Thread 197\n",
            "GPU active.....Thread 198\n",
            "GPU active.....Thread 199\n",
            "GPU active.....Thread 200\n",
            "GPU active.....Thread 201\n",
            "GPU active.....Thread 202\n",
            "GPU active.....Thread 203\n",
            "GPU active.....Thread 204\n",
            "GPU active.....Thread 205\n",
            "GPU active.....Thread 206\n",
            "GPU active.....Thread 207\n",
            "GPU active.....Thread 208\n",
            "GPU active.....Thread 209\n",
            "GPU active.....Thread 210\n",
            "GPU active.....Thread 211\n",
            "GPU active.....Thread 212\n",
            "GPU active.....Thread 213\n",
            "GPU active.....Thread 214\n",
            "GPU active.....Thread 215\n",
            "GPU active.....Thread 216\n",
            "GPU active.....Thread 217\n",
            "GPU active.....Thread 218\n",
            "GPU active.....Thread 219\n",
            "GPU active.....Thread 220\n",
            "GPU active.....Thread 221\n",
            "GPU active.....Thread 222\n",
            "GPU active.....Thread 223\n",
            "GPU active.....Thread 224\n",
            "GPU active.....Thread 225\n",
            "GPU active.....Thread 226\n",
            "GPU active.....Thread 227\n",
            "GPU active.....Thread 228\n",
            "GPU active.....Thread 229\n",
            "GPU active.....Thread 230\n",
            "GPU active.....Thread 231\n",
            "GPU active.....Thread 232\n",
            "GPU active.....Thread 233\n",
            "GPU active.....Thread 234\n",
            "GPU active.....Thread 235\n",
            "GPU active.....Thread 236\n",
            "GPU active.....Thread 237\n",
            "GPU active.....Thread 238\n",
            "GPU active.....Thread 239\n",
            "GPU active.....Thread 240\n",
            "GPU active.....Thread 241\n",
            "GPU active.....Thread 242\n",
            "GPU active.....Thread 243\n",
            "GPU active.....Thread 244\n",
            "GPU active.....Thread 245\n",
            "GPU active.....Thread 246\n",
            "GPU active.....Thread 247\n",
            "GPU active.....Thread 248\n",
            "GPU active.....Thread 249\n",
            "GPU active.....Thread 250\n",
            "GPU active.....Thread 251\n",
            "GPU active.....Thread 252\n",
            "GPU active.....Thread 253\n",
            "GPU active.....Thread 254\n",
            "GPU active.....Thread 255\n",
            "GPU active.....Thread 0\n",
            "GPU active.....Thread 1\n",
            "GPU active.....Thread 2\n",
            "GPU active.....Thread 3\n",
            "GPU active.....Thread 4\n",
            "GPU active.....Thread 5\n",
            "GPU active.....Thread 6\n",
            "GPU active.....Thread 7\n",
            "GPU active.....Thread 8\n",
            "GPU active.....Thread 9\n",
            "GPU active.....Thread 10\n",
            "GPU active.....Thread 11\n",
            "GPU active.....Thread 12\n",
            "GPU active.....Thread 13\n",
            "GPU active.....Thread 14\n",
            "GPU active.....Thread 15\n",
            "GPU active.....Thread 16\n",
            "GPU active.....Thread 17\n",
            "GPU active.....Thread 18\n",
            "GPU active.....Thread 19\n",
            "GPU active.....Thread 20\n",
            "GPU active.....Thread 21\n",
            "GPU active.....Thread 22\n",
            "GPU active.....Thread 23\n",
            "GPU active.....Thread 24\n",
            "GPU active.....Thread 25\n",
            "GPU active.....Thread 26\n",
            "GPU active.....Thread 27\n",
            "GPU active.....Thread 28\n",
            "GPU active.....Thread 29\n",
            "GPU active.....Thread 30\n",
            "GPU active.....Thread 31\n",
            "GPU active.....Thread 32\n",
            "GPU active.....Thread 33\n",
            "GPU active.....Thread 34\n",
            "GPU active.....Thread 35\n",
            "GPU active.....Thread 36\n",
            "GPU active.....Thread 37\n",
            "GPU active.....Thread 38\n",
            "GPU active.....Thread 39\n",
            "GPU active.....Thread 40\n",
            "GPU active.....Thread 41\n",
            "GPU active.....Thread 42\n",
            "GPU active.....Thread 43\n",
            "GPU active.....Thread 44\n",
            "GPU active.....Thread 45\n",
            "GPU active.....Thread 46\n",
            "GPU active.....Thread 47\n",
            "GPU active.....Thread 48\n",
            "GPU active.....Thread 49\n",
            "GPU active.....Thread 50\n",
            "GPU active.....Thread 51\n",
            "GPU active.....Thread 52\n",
            "GPU active.....Thread 53\n",
            "GPU active.....Thread 54\n",
            "GPU active.....Thread 55\n",
            "GPU active.....Thread 56\n",
            "GPU active.....Thread 57\n",
            "GPU active.....Thread 58\n",
            "GPU active.....Thread 59\n",
            "GPU active.....Thread 60\n",
            "GPU active.....Thread 61\n",
            "GPU active.....Thread 62\n",
            "GPU active.....Thread 63\n",
            "Max error: 392.000000\n",
            "0.00\n",
            "4.00\n",
            "8.00\n",
            "12.00\n",
            "16.00\n",
            "20.00\n",
            "24.00\n",
            "28.00\n",
            "32.00\n",
            "36.00\n",
            "40.00\n",
            "44.00\n",
            "48.00\n",
            "52.00\n",
            "56.00\n",
            "60.00\n",
            "64.00\n",
            "68.00\n",
            "72.00\n",
            "76.00\n",
            "80.00\n",
            "84.00\n",
            "88.00\n",
            "92.00\n",
            "96.00\n",
            "100.00\n",
            "104.00\n",
            "108.00\n",
            "112.00\n",
            "116.00\n",
            "120.00\n",
            "124.00\n",
            "128.00\n",
            "132.00\n",
            "136.00\n",
            "140.00\n",
            "144.00\n",
            "148.00\n",
            "152.00\n",
            "156.00\n",
            "160.00\n",
            "164.00\n",
            "168.00\n",
            "172.00\n",
            "176.00\n",
            "180.00\n",
            "184.00\n",
            "188.00\n",
            "192.00\n",
            "196.00\n",
            "200.00\n",
            "204.00\n",
            "208.00\n",
            "212.00\n",
            "216.00\n",
            "220.00\n",
            "224.00\n",
            "228.00\n",
            "232.00\n",
            "236.00\n",
            "240.00\n",
            "244.00\n",
            "248.00\n",
            "252.00\n",
            "256.00\n",
            "260.00\n",
            "264.00\n",
            "268.00\n",
            "272.00\n",
            "276.00\n",
            "280.00\n",
            "284.00\n",
            "288.00\n",
            "292.00\n",
            "296.00\n",
            "300.00\n",
            "304.00\n",
            "308.00\n",
            "312.00\n",
            "316.00\n",
            "320.00\n",
            "324.00\n",
            "328.00\n",
            "332.00\n",
            "336.00\n",
            "340.00\n",
            "344.00\n",
            "348.00\n",
            "352.00\n",
            "356.00\n",
            "360.00\n",
            "364.00\n",
            "368.00\n",
            "372.00\n",
            "376.00\n",
            "380.00\n",
            "384.00\n",
            "388.00\n",
            "392.00\n",
            "396.00\n",
            "\n"
          ]
        }
      ]
    }
  ]
}